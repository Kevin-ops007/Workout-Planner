{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import regex as re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time \n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "import warnings\n",
    "\n",
    "load_dotenv()\n",
    "warnings.simplefilter(action='ignore', category=(FutureWarning, DeprecationWarning, InsecureRequestWarning))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mount_session():\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \\\n",
    "        (KHTML, like Gecko) Ubuntu Chromium/80.0.3987.163 Chrome/80.0.3987.163 Safari/537.36',\n",
    "        'Accept-Language': 'en-US,en;q=0.9,es;q=0.8,pt;q=0.7'\n",
    "    }\n",
    "\n",
    "    session = requests.Session()\n",
    "    retry = Retry(connect=3, backoff_factor=0.5)\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "\n",
    "    return session, headers\n",
    "\n",
    "def fix_base_prefix(url):\n",
    "    prefix = 'https://exrx.net/Lists/'\n",
    "    if url.startswith(prefix):\n",
    "        return url\n",
    "    return prefix + url\n",
    "\n",
    "def fix_index_prefix(url):\n",
    "    prefix = 'https://exrx.net/'\n",
    "    if not url.startswith(prefix):\n",
    "        url = url.replace(\"../../\", prefix)\n",
    "    return url\n",
    "\n",
    "def scrape_base(url):\n",
    "    def filter(to_filter):\n",
    "        pattern = re.compile(r'.*ExList\\/[A-Za-z]+\\#*[A-Za-z]+')\n",
    "\n",
    "        if re.search(pattern, str(to_filter)):\n",
    "            return True \n",
    "        return False\n",
    "    \n",
    "    session, headers = mount_session()\n",
    "    \n",
    "    page = session.get(url, headers=headers, verify=False)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    links = []\n",
    "    for link in soup.find_all('a'):\n",
    "        if filter(link):\n",
    "            links.append(link.get('href'))\n",
    "\n",
    "    return links\n",
    "\n",
    "def scrape_instance(url):\n",
    "    def filter(to_filter):\n",
    "        pattern = re.compile(r'.*WeightExercises\\/[A-Za-z]+\\/[A-Za-z]+')\n",
    "\n",
    "        if re.search(pattern, str(to_filter)):\n",
    "            return True \n",
    "        return False\n",
    "    \n",
    "    session, headers = mount_session()\n",
    "\n",
    "    links = []\n",
    "    try:\n",
    "        page = session.get(url, headers=headers, verify=False)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "        for link in soup.find_all('a'):\n",
    "            if filter(link):\n",
    "                links.append(link.get('href'))\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        time.sleep(3)\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://exrx.net/Lists/Directory'\n",
    "links = scrape_base(base_url)\n",
    "standardized_links = [fix_base_prefix(link) for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_indexes = []\n",
    "with tqdm(total=len(standardized_links)) as pbar:\n",
    "    for link in standardized_links:\n",
    "        total_indexes.extend(scrape_instance(link))\n",
    "        pbar.update(1)\n",
    "total_indexes = list(set(total_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_indexes = [fix_index_prefix(link) for link in total_indexes]\n",
    "# Remove duplicates\n",
    "standardized_indexes = list(set(standardized_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def account_login(url, driver):\n",
    "    # Create a .env file with your uName, and uPassword variables\n",
    "    uName = os.getenv(\"uName\")\n",
    "    uPassword = os.getenv(\"uPassword\")\n",
    "\n",
    "    driver.get(url)\n",
    "    driver.find_element(\"name\", \"uName\").send_keys(uName)\n",
    "    driver.find_element(\"name\", \"uPassword\").send_keys(uPassword)\n",
    "    driver.find_element(\"name\", \"uPassword\").send_keys(Keys.RETURN)\n",
    "\n",
    "def mount_driver():\n",
    "    options = Options()\n",
    "    options.add_argument('--headless=new')\n",
    "    options.add_argument(\n",
    "        '--user-agent=Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 \\\n",
    "        (KHTML, like Gecko) Ubuntu Chromium/80.0.3987.163 Chrome/80.0.3987.163 Safari/537.36')\n",
    "    options.add_argument('user-data-dir=C:/Users/bhava/Documents/GitHub/Workout-Planner/profile')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    account_login('https://exrx.net/login', driver)\n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_and_replace(muscles):\n",
    "    if len(muscles) in [0, 1]:\n",
    "        return muscles\n",
    "\n",
    "    cleaned_muscles = []\n",
    "    reversed_muscles = muscles[::-1]\n",
    "\n",
    "    embedded = reversed_muscles[0]\n",
    "    for i in range(len(reversed_muscles)):\n",
    "        if i == 0:\n",
    "            cleaned_muscles.append(reversed_muscles[i])\n",
    "            continue\n",
    "\n",
    "        cleaned_muscles.append(reversed_muscles[i].replace(embedded, \"\"))\n",
    "        embedded = reversed_muscles[i]\n",
    "    return cleaned_muscles[::-1]\n",
    "\n",
    "def extract_muscles(soup, section_name):\n",
    "    muscle_ul = soup.find(\"strong\", text=section_name)\n",
    "    muscles = []\n",
    "\n",
    "    try:\n",
    "        muscle_ul = muscle_ul.next_element.next_element\n",
    "        # Edge case for Dynamic Stabilizers and Antagonist Stabilizers where we're one element off\n",
    "        if muscle_ul in [\"Dynamic Stabilizers\", \"Antagonist Stabilizers\"]:\n",
    "            muscle_ul = muscle_ul.next_element\n",
    "        for i in muscle_ul.find_all(\"li\"):\n",
    "            muscles.append(i.text)\n",
    "    except (AttributeError, TypeError):\n",
    "        muscles.append(muscle_ul.text if muscle_ul else '')\n",
    "    muscles = remove_and_replace(muscles)\n",
    "    return muscles\n",
    "\n",
    "def extract_exercise(soup):\n",
    "    try:\n",
    "        exercise = soup.find(\"h1\").text\n",
    "    except AttributeError:\n",
    "        exercise = \"\"\n",
    "    return exercise\n",
    "\n",
    "def extract_classifications(soup):\n",
    "    classifications = []\n",
    "\n",
    "    try:\n",
    "        table = soup.find(\"table\")\n",
    "        rows = table.find_all(\"tr\")\n",
    "\n",
    "        for row in rows:\n",
    "            cells = row.find_all(\"td\")\n",
    "            for cell in cells:\n",
    "                classifications.append(cell.text.replace(\":\", \"\"))\n",
    "        classifications = remove_and_replace(classifications)\n",
    "    except AttributeError:\n",
    "        classifications = [\"\", \"\", \"\", \"\", \"\", \"\"]\n",
    "    return classifications[:6]\n",
    "\n",
    "def scrape_info(link, driver):\n",
    "    session, headers = mount_session()\n",
    "    page = session.get(link, headers=headers, verify=False)\n",
    "\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "    title = extract_exercise(soup)\n",
    "    if not title:\n",
    "        return None\n",
    "    \n",
    "    if title == \"Premium Content\":\n",
    "        driver.get(link)\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        title = extract_exercise(soup)\n",
    "\n",
    "    classifications = extract_classifications(soup)\n",
    "\n",
    "    target_muscles = extract_muscles(soup, \"Target\")\n",
    "    synergist_muscles = extract_muscles(soup, \"Synergists\")\n",
    "    stabilizer_muscles = extract_muscles(soup, \"Stabilizers\")\n",
    "    dynamic_stabilizer_muscles = extract_muscles(soup, \"Dynamic Stabilizers\")\n",
    "    antagonist_stabilizer_muscles = extract_muscles(soup, \"Antagonist Stabilizers\")\n",
    "\n",
    "    data = {\n",
    "        \"exercise\": title,\n",
    "        \"utility\": classifications[1],\n",
    "        \"mechanics\": classifications[3],\n",
    "        \"force\": classifications[5],\n",
    "        \"target_muscles\": target_muscles,\n",
    "        \"synergist_muscles\": synergist_muscles,\n",
    "        \"stabilizer_muscles\": stabilizer_muscles,\n",
    "        \"dynamic_stabilizer_muscles\": dynamic_stabilizer_muscles,\n",
    "        \"antagonist_stabilizer_muscles\": antagonist_stabilizer_muscles\n",
    "    }\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "driver = mount_driver()\n",
    "with tqdm(total=len(standardized_indexes)) as pbar:\n",
    "    for link in standardized_indexes:\n",
    "        data = scrape_info(link, driver)\n",
    "        if data:\n",
    "            data_list.append(data)\n",
    "        pbar.update(1)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_list)\n",
    "df.to_csv(\"exrx.csv\", index=False)\n",
    "df.head(n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
